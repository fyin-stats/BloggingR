---
title: "A self-learning note for statistical decision theory - part 1"
author: "Fan Yin"
date: '2020-12-23T21:13:14-05:00'
output: html_document
categories: Statistics
tags:
- Statistical Decision Theory
- Bayesian Analysis
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

As a statistician who often solves problems from a frequentist perspective but with a Bayesian soul (allegedly), statistical decision theory has always been an intriguing topic to me. Thanks to the holiday season, I finally got a chance to revisit this topic and I hope I can build up a more systematic understanding of it this time. To facilitate my mastery of the knowledge, I plan to learn by writing and sharing. As the very first step of this project, Iâ€™d like to cover the basic elements here in this blog post, including - (i) Motivation: what values can statistical decision theory offer? (ii) Notations and key concepts; (iii) Optimality criteria - how do we make decisions?

## Motivation

The statistical decision theory, as noted by Dr. James O. Berger in his book "Statistical Decision Theory and Bayesian Analysis", is concerned with the problem of making decisions in the presence of statistical knowledge which can help explain some of the uncertainties involved in the decision-making problem. Typically, the uncertainties are assumed to be represented by unknown quantities (parameters) - for example, the proportion of people for which the drug will prove effective, $\theta$, which is generally unknown, and one must conduct experiments to obtain statistical information about it. The classical statistics targets at making inference about the parameters based on the sample information only, whereas the statistical decision theory is aimed at combing the sample information with other relevant knowledge (including possible consequences associated with different decisions, a-priori knowledge about the possible values of parameters) to make the \textit{optimal} decision.

<!-- # ```{r cars} -->
<!-- # summary(cars) -->
<!-- # ``` -->

## Notations and Key Concepts

\begin{itemize}
\item Observed data: $X$
\item An action: $a \in \mathcal{A}$
\item A decision function: $a = \delta(X)$
\item Parameters (state of the world): $\theta \in \Theta$
\item A loss function: $L(a, \theta)$, where $L(a, \theta) \geqslant 0, \forall a \in \mathcal{A}, \theta \in \Theta$
\item Data generating model: $f(X|\theta)$
\end{itemize}

After observing the data $X$, the decision-maker picks a decision that minimizes the loss $L(\theta, a)$, for the unknown parameters $\theta$. For example, the most commonly used loss functions are listed below 

\begin{itemize}
\item Squared error: $L(\theta, \delta(X)) = (\theta - \delta(X))^2$
\item Absolute error: $L(\theta, \delta(X)) = |\theta - \delta(X)|$
<!-- \item  -->
<!-- \begin{align*} -->
<!-- \mathcal{A}_{\epsilon, m}(x) = & \begin{cases} -->
<!--     1, & \text{with probability} \ \frac{1}{e^{\epsilon} + 1} + \frac{x}{m} \cdot \frac{e^{\epsilon} - 1}{e^{\epsilon} + 1} \\ -->
<!--     0, & \text{otherwise} -->
<!--   \end{cases} -->
<!-- \end{align*} -->
\end{itemize}

<!-- You can also embed plots, for example: -->

<!-- ```{r pressure, echo=FALSE} -->
<!-- plot(pressure) -->
<!-- ``` -->

<!-- Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot. -->
